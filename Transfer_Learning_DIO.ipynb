{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOpIH20hfzJs/AQHPp+GfaM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/V4lciJr/Trasnfer-Learning-DIO/blob/main/Transfer_Learning_DIO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transferência de Aprendizado / Ajuste Fino\n",
        "Este tutorial vai guiá-lo pelo processo de uso de transferência de aprendizado para aprender um classificador de imagens preciso a partir de um número relativamente pequeno de amostras de treinamento. De maneira geral, transferência de aprendizado refere-se ao processo de aproveitar o conhecimento aprendido em um modelo para treinar outro modelo.\n",
        "\n",
        "Mais especificamente, o processo envolve pegar uma rede neural existente que foi treinada com bom desempenho em um conjunto de dados maior e usá-la como base para um novo modelo, que aproveita a precisão do modelo anterior para uma nova tarefa. Este método se tornou popular nos últimos anos para melhorar o desempenho de uma rede neural treinada em um pequeno conjunto de dados; a intuição é que o novo conjunto de dados pode ser pequeno demais para ser treinado com bom desempenho por si só, mas sabemos que a maioria das redes neurais treinadas para aprender características de imagens acabam aprendendo características semelhantes, especialmente nas camadas iniciais, que são mais genéricas (detectores de borda, manchas, e assim por diante).\n",
        "\n",
        "A transferência de aprendizado foi amplamente viabilizada pela publicação de modelos de última geração; para os modelos de melhor desempenho em tarefas de classificação de imagens (como o [ILSVRC](http://www.image-net.org/challenges/LSVRC/)), é prática comum agora não apenas publicar a arquitetura, mas também liberar os pesos treinados do modelo. Isso permite que amadores usem esses classificadores de imagem de topo para melhorar o desempenho de seus próprios modelos específicos de tarefa.\n",
        "\n",
        "## Extração de Características vs. Ajuste Fino\n",
        "\n",
        "Em um extremo, a transferência de aprendizado pode envolver pegar a rede pré-treinada e congelar os pesos, usando uma de suas camadas ocultas (geralmente a última) como um extrator de características, utilizando essas características como entrada para uma rede neural menor.\n",
        "\n",
        "No outro extremo, começamos com a rede pré-treinada, mas permitimos que alguns dos pesos (geralmente a última camada ou as últimas camadas) sejam modificados. Outro nome para este procedimento é \"ajuste fino\" (fine-tuning), porque estamos ajustando ligeiramente os pesos da rede pré-treinada para a nova tarefa. Normalmente, treinamos essa rede com uma taxa de aprendizado mais baixa, pois esperamos que as características já sejam relativamente boas e não precisem ser alteradas demais.\n",
        "\n",
        "Às vezes, fazemos algo intermediário: congelamos apenas as camadas iniciais/genéricas e ajustamos as camadas finais. Qual estratégia é melhor depende do tamanho do seu conjunto de dados, do número de classes e de quão semelhante o conjunto de dados anterior é ao novo (e, portanto, se ele pode se beneficiar dos mesmos extratores de características aprendidos). Uma discussão mais detalhada sobre como decidir pode ser encontrada em [[1]](http://cs231n.github.io/transfer-learning/) [[2]](http://sebastianruder.com/transfer-learning/).\n",
        "\n",
        "## Procedimento\n",
        "\n",
        "Neste guia, vamos passar pelo processo de carregar um classificador de imagem de última geração, [VGG16](https://arxiv.org/pdf/1409.1556.pdf), que venceu o desafio ImageNet em 2014 [link](http://www.robots.ox.ac.uk/~vgg/research/very_deep), e usá-lo como um extrator de características fixo para treinar um classificador personalizado em nossas próprias imagens. No entanto, com poucas alterações de código, você pode tentar o ajuste fino também.\n",
        "\n",
        "Vamos primeiro carregar o VGG16 e remover sua camada final, a camada softmax de 1000 classes específica para o ImageNet, substituindo-a por uma nova camada de classificação para as classes que estamos treinando. Em seguida, vamos congelar todos os pesos na rede, exceto os novos pesos que conectam à nova camada de classificação, e depois treinar essa nova camada de classificação em nosso novo conjunto de dados.\n",
        "\n",
        "Também vamos comparar esse método com o treinamento de uma rede neural pequena do zero no novo conjunto de dados. Como veremos, isso vai melhorar dramaticamente nossa precisão. Vamos fazer essa parte primeiro.\n",
        "\n",
        "Como nosso conjunto de dados de teste, vamos usar um dataset com cerca de 6000 imagens pertencentes a 97 classes e treinar um classificador de imagens com cerca de 80% de precisão. Vale ressaltar que essa estratégia escala bem para conjuntos de imagens onde você pode ter até mesmo apenas algumas centenas de imagens ou menos. Seu desempenho será menor com um número pequeno de amostras (dependendo das classes), como de costume, mas ainda assim impressionante considerando as limitações habituais.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y8QiOfWIdlb5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aGXKULnZdaoH"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "\n",
        "#if using Theano with GPU\n",
        "#os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "_CITATION = \"\"\"\\\n",
        "@Inproceedings (Conference){asirra-a-captcha-that-exploits-interest-aligned-manual-image-categorization,\n",
        "author = {Elson, Jeremy and Douceur, John (JD) and Howell, Jon and Saul, Jared},\n",
        "title = {Asirra: A CAPTCHA that Exploits Interest-Aligned Manual Image Categorization},\n",
        "booktitle = {Proceedings of 14th ACM Conference on Computer and Communications Security (CCS)},\n",
        "year = {2007},\n",
        "month = {October},\n",
        "publisher = {Association for Computing Machinery, Inc.},\n",
        "url = {https://www.microsoft.com/en-us/research/publication/asirra-a-captcha-that-exploits-interest-aligned-manual-image-categorization/},\n",
        "edition = {Proceedings of 14th ACM Conference on Computer and Communications Security (CCS)},\n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting a dataset"
      ],
      "metadata": {
        "id": "ng7kjqOvL7mx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurações iniciais\n",
        "batch_size = 32\n",
        "image_size = (150, 150)\n",
        "base_dir = \"./datasets\"  # Ajuste para o diretório em que você armazenará o dataset\n"
      ],
      "metadata": {
        "id": "aTPvbe7gj91G"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Baixar o dataset\n",
        "if not os.path.exists(base_dir):\n",
        "    tf.keras.utils.get_file(\n",
        "        fname=\"cats_and_dogs.zip\",\n",
        "        origin=\"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\",\n",
        "        extract=True,\n",
        "        cache_dir=\"./\"\n",
        "    )"
      ],
      "metadata": {
        "id": "-Gl6LWJgSOMX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0287abeb-78d7-402b-9448-9fd9c1c88348"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\n",
            "\u001b[1m824887076/824887076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Diretórios de treino, validação e teste\n",
        "train_dir = os.path.join(base_dir, 'PetImages')\n",
        "dogs_dir = os.path.join(train_dir, 'Dog')\n",
        "cats_dir = os.path.join(train_dir, 'Cat')"
      ],
      "metadata": {
        "id": "iLSZ7UNsVU6u"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remover arquivos corrompidos\n",
        "num_skipped = 0\n",
        "for folder_name in (\"Dog\", \"Cat\"):\n",
        "    folder_path = os.path.join(train_dir, folder_name)\n",
        "    print(folder_path)\n",
        "    for fname in os.listdir(folder_path):\n",
        "        fpath = os.path.join(folder_path, fname)\n",
        "        try:\n",
        "            img = tf.keras.utils.load_img(fpath)\n",
        "        except:\n",
        "            num_skipped += 1\n",
        "            os.remove(fpath)\n",
        "print(f\"Removidos {num_skipped} arquivos corrompidos.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seLA_MbIft4z",
        "outputId": "011b6229-eaa2-4214-fa83-c9083ae1558b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./datasets/PetImages/Dog\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./datasets/PetImages/Cat\n",
            "Removidos 4 arquivos corrompidos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Geradores de dados com augmentação\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    validation_split=0.2,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ydhFGWofweh",
        "outputId": "b221478a-b3d2-4580-b0c5-0c3a46f411dc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 images belonging to 2 classes.\n",
            "Found 4998 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o modelo base (VGG16)\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "s1cILuXEj1R9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deeb909b-6ac1-4e83-fb75-7385930952db"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Construir o modelo de Transfer Learning\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "xLKSbPx1j4RU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinar o modelo\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=10\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMwG5JMrj8T2",
        "outputId": "ba43cc16-7bc5-48fb-b770-7e406640c838"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m101/625\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:58\u001b[0m 227ms/step - accuracy: 0.6434 - loss: 0.8562"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 293ms/step - accuracy: 0.7641 - loss: 0.5322 - val_accuracy: 0.8479 - val_loss: 0.3341\n",
            "Epoch 2/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 296ms/step - accuracy: 0.8429 - loss: 0.3594 - val_accuracy: 0.8713 - val_loss: 0.2867\n",
            "Epoch 3/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 285ms/step - accuracy: 0.8488 - loss: 0.3480 - val_accuracy: 0.8717 - val_loss: 0.2926\n",
            "Epoch 4/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 285ms/step - accuracy: 0.8617 - loss: 0.3236 - val_accuracy: 0.8752 - val_loss: 0.2911\n",
            "Epoch 5/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 294ms/step - accuracy: 0.8571 - loss: 0.3182 - val_accuracy: 0.8675 - val_loss: 0.3060\n",
            "Epoch 6/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 284ms/step - accuracy: 0.8656 - loss: 0.3089 - val_accuracy: 0.8780 - val_loss: 0.2868\n",
            "Epoch 7/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 285ms/step - accuracy: 0.8619 - loss: 0.3194 - val_accuracy: 0.8729 - val_loss: 0.2820\n",
            "Epoch 8/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 283ms/step - accuracy: 0.8686 - loss: 0.3017 - val_accuracy: 0.8754 - val_loss: 0.2744\n",
            "Epoch 9/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 295ms/step - accuracy: 0.8664 - loss: 0.3042 - val_accuracy: 0.8810 - val_loss: 0.2687\n",
            "Epoch 10/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 284ms/step - accuracy: 0.8635 - loss: 0.3071 - val_accuracy: 0.8737 - val_loss: 0.2813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliar o modelo\n",
        "test_loss, test_accuracy = model.evaluate(validation_generator)\n",
        "print(f\"Test accuracy: {test_accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBW5c1aYj--V",
        "outputId": "f4a657c9-899e-4070-b2fd-5f87cb754b02"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 230ms/step - accuracy: 0.8709 - loss: 0.2766\n",
            "Test accuracy: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvar o modelo\n",
        "model.save(\"cats_vs_dogs_transfer_learning.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZHe3b4xPHxG",
        "outputId": "b94fe0a2-4b25-484d-9104-6b02650fc804"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    }
  ]
}